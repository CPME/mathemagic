import os
import re
import io
import sys
import contextlib
from pathlib import Path
from typing import Tuple, Optional, Dict, List

import anthropic
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Get API key from environment variables
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
if not ANTHROPIC_API_KEY:
    raise ValueError("ANTHROPIC_API_KEY environment variable is not set")

# Initialize Anthropic client
client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

def load_system_prompt(prompt_file: str = "system_instruction.md") -> str:
    """Load a system prompt from the specified file."""
    current_dir = Path(__file__).parent
    system_instruction_path = current_dir / prompt_file
    
    try:
        with open(system_instruction_path, "r") as file:
            return file.read()
    except FileNotFoundError:
        # Fallback to a basic system prompt if the file is not found
        return """You are a helpful assistant that converts natural language math problems into Python code.
Please solve the math problem by writing Python code. Use the Pint library for unit conversions.
Return only the Python code without any explanations."""

# Load system prompts once when module is imported
UNIT_SYSTEM_PROMPT = load_system_prompt("system_instruction.md")
SYMBOLIC_SYSTEM_PROMPT = load_system_prompt("sys_prompt_symbolic.md")

# System prompt summaries for selection
SYSTEM_PROMPT_SUMMARIES = {
    "unit": "This system prompt guides the AI to generate Python code with Pint for unit-aware calculations.",
    "symbolic": "This system prompt guides the AI to generate Python code with SymPy for symbolic mathematical calculations."
}

def select_system_prompt(user_prompt: str) -> str:
    """
    Select the appropriate system prompt based on the user's input.
    
    Args:
        user_prompt: The user's math problem or question
        
    Returns:
        The selected system prompt
    """
    # Create a prompt for the LLM to decide which system prompt to use
    selection_prompt = f"""
Based on the following user query, determine whether it requires:
1. Unit-aware calculations (using Pint) - for problems involving physical quantities with units
2. Symbolic mathematics (using SymPy) - for algebraic manipulation, calculus, equations, etc.

Unit calculations prompt: {SYSTEM_PROMPT_SUMMARIES['unit']}
Symbolic mathematics prompt: {SYSTEM_PROMPT_SUMMARIES['symbolic']}

User query: "{user_prompt}"

Reply with ONLY ONE of these exact words: "unit" or "symbolic"
"""
    
    # Call Anthropic API to decide
    message = client.messages.create(
        model="claude-3-haiku-20240307",  # Using a smaller model for efficiency
        max_tokens=10,
        system="You are a helpful assistant that categorizes math problems.",
        messages=[
            {"role": "user", "content": selection_prompt}
        ]
    )
    
    # Extract the response
    response = message.content[0].text.strip().lower()
    
    # Select the appropriate system prompt
    if "symbolic" in response:
        return SYMBOLIC_SYSTEM_PROMPT
    else:
        return UNIT_SYSTEM_PROMPT

def prompt_to_py(user_prompt: str, system_prompt: Optional[str] = None) -> str:
    """
    Convert a user prompt to Python code using the Anthropic API.
    
    Args:
        user_prompt: The user's math problem or question
        system_prompt: Optional system prompt to override the default
        
    Returns:
        The Python code generated by the LLM
    """
    if system_prompt is None:
        system_prompt = select_system_prompt(user_prompt)
    
    # Combine system prompt and user prompt
    full_prompt = f"{user_prompt}"
    
    # Call Anthropic API
    message = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1000,
        system=system_prompt,
        messages=[
            {"role": "user", "content": full_prompt}
        ]
    )
    
    # Extract the response
    response = message.content[0].text
    
    return response

def extract_python_code(response: str) -> str:
    """
    Extract Python code from the LLM response.
    
    Args:
        response: The response from the LLM
        
    Returns:
        The extracted Python code
    """
    # Look for Python code blocks in the response
    pattern = r"```python\s*(.*?)\s*```"
    matches = re.findall(pattern, response, re.DOTALL)
    
    if matches:
        return matches[0]
    else:
        # If no code block is found, assume the entire response is code
        return response

def execute_py(response: str) -> Tuple[str, bool]:
    """
    Execute the Python code extracted from the response.
    
    Args:
        response: The response containing Python code
        
    Returns:
        A tuple containing (execution_result, success_flag)
    """
    # Extract Python code
    code = extract_python_code(response)
    
    # Capture stdout to get the result
    stdout_capture = io.StringIO()
    success = True
    
    try:
        # Redirect stdout to capture print statements
        with contextlib.redirect_stdout(stdout_capture):
            # Determine if the code is likely using SymPy or Pint based on imports
            if "sympy" in code.lower():
                sympy_imports = """
from sympy import symbols, solve, simplify, Eq, diff, integrate, limit, Matrix, sin, cos, tan, exp, log, pi, oo, Symbol, Function
from sympy import init_printing
init_printing(use_unicode=True)
"""
                # Execute the code with SymPy imports
                exec(sympy_imports + code)
            else:
                # Default to Pint imports
                pint_imports = """
from pint import UnitRegistry
ureg = UnitRegistry()
Q_ = ureg.Quantity
"""
                # Execute the code with Pint imports
                exec(pint_imports + code)
            
        result = stdout_capture.getvalue().strip()
        if not result:
            result = "Code executed successfully but produced no output."
    except Exception as e:
        result = f"Error executing code: {str(e)}"
        success = False
    
    return result, success
